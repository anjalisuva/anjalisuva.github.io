---
title: "Child Welfare Ethics Exploration"
description: |
  Exploration of ethical dilemma involving algorithmic tool used in Allegheny County's child welfare system.   
author: Anjali Suva
date: November 12, 2025
format: html
---

This project is an case study exploring the ethics involved within the field of data science. No code was used for this project.

The Issue:

In Allegheny County, Pennsylvania, the Department of Human Services (DHS) utilized an algorithmic modelling tool used to predict the risk that a child will be placed in foster care two years after investigation. The Allegheny Family Screening Tool (AFST), uses personal data regarding birth, Medicaid, substance abuse, mental health, jail and probation records, among other government data sets (Ho & Burke, 2022). Using detailed personal data, the algorithm generates a risk score from 1 to 20 for each child, with 1 indicating low risk and 20 indicating very high risk. The tool is used for cases of neglect, which are separate from physical and sexual abuse and can include anything from inadequate housing to poor hygiene. 

Critics of the algorithm cite its encoding of racial bias as evidence of its defects. According to research done by a team at Carnegie Mellon University, AFST demonstrated a pattern of flagging black children for mandatory neglect investigations at disproportionate rates compared to white children. The team also discovered that for approximately a third of the AFST’s risk scores contradicted social workers’ assessments.

Several welfare workers interviewed about the algorithm have suggested that regardless of the developers objective attempt to assess families, racial bias inherent in the data systems used to design the algorithm will affect the way the algorithm evaluates families (Ho & Burke, 2022). This means that black families are more likely to receive higher risk scores from the screening tool and are thus more likely to face scrutiny by social workers. In a worst case scenario, overreliance on the tool can lead to unnecessarily separating children from their families.

Supporters of the algorithm include the county and the developers of the algorithm, who claim that the high stakes involved in child welfare necessitate its use. On the one hand, failing to adequately flag and investigate a child’s situation could result in death. On the other hand, unnecessarily investigating families could set them up for separation. The county and developers of AFST claim that the algorithm can help separate reports of families that require intervention from baseless reports. According to the director of the DHS, Erin Dalton, the algorithm is not meant to be the sole determinant of which family gets investigated. In a response to the article written by Ho and Burke, Dalton writes that child welfare workers consider the AFST as just one element, and that they must “weigh all the other information that is not included in the AFST such as the specific allegation, reporter, call narrative, and the screener’s assessment of risk and safety to determine the appropriate screening response” (Dalton, 2022). Thus, the final say on who gets investigated lies with the worker, not the algorithm. 

The ethical dilemma in this project involves whether or not an algorithm meant for saving children facing neglect is causing more harm than good for the aforementioned reasons. Although the social worker is the ultimate decision-maker in evaluating the validity of the score and determining which families get investigated, the fact that racial bias is encoded within the algorithm means that families are facing undue stress over potentially being investigated without cause.

Questions/Principles to Consider:

What was the data collection process? Were the observations collected ethically? Are there missing observations?

According to the Allegheny County website, the data is collected from the county’s Department of Human Services (DHS) Data Warehouse which includes publicly-funded human services data (e.g., behavioral health, child welfare, intellectual disability, homelessness and aging) and has been expanded to include other sources (DHS Data Warehouse, 2024). The sources specifically are not identified, but according to Ho and Burke (2022), one controversial element of the original version of the algorithm was that it included information on whether or not a family received welfare dollars or food stamps. The latest version of the algorithm does not include such information, but little transparency remains over what information is included. Based on the information available, the data for the algorithm is not pulled nor used with the knowing consent of the families being investigated. Although the data in general may have been collected in a neutral or ethical manner—i.e. collected when families register to receive certain resources—its specific use in the algorithm is not. The data within the several databases that the algorithm pulls from is kept private from families and the general public, so whether or not there are missing observations within the data sets is unknown. 

Does AFST align with principle No. 4 in the Data Values and Principles Manifesto, prioritizing the continuous collection and availability of discussions and metadata?

The Allegheny Family Screening Tool does not promote transparency as outlined in the above principle. Instead, families are kept in the dark about the role that the tool plays in investigations and how much sway it holds in court hearings. One reason for this is that family court hearings are closed to the public and records are sealed. As a result, Ho and Burke (2022) describes how they were unable to obtain first-hand accounts of families who were investigated for neglect due to the algorithm nor cases that led to a child being sent to foster care. Families are also not shown the scores that the algorithm assigns them, meaning that parents may experience a psychological toll and anxiety from being unaware of whether or not a certain action (such as signing up for government programs) may raise or lower their score. 

Does ASFT align with principle No. 7 in the Data Values and Principles Manifesto, recognizing and mitigating bias in the data used?

As demonstrated by the Carnegie Mellon University team’s research, the algorithm reinforces racial disparities within the child welfare system. The rate at which black children were flagged for mandatory neglect investigations was disproportionate compared to the rate at which white children were flagged. This is further supported by Allegheny County, which provided research confirming that AFST does not help with combating disparities in the rates at which black and white child neglect cases are examined (Ho & Burke, 2022). Because the data utilized in the algorithm is often biased against Black families, who are more likely to be affected by discriminatory policies, the data that is utilized by the algorithms to make predictions on which children are most at risk for neglect will inherently be biased. The algorithm does not take as a variable in determining which family is at risk, but many other variables can act as a proxy for race. This includes data on families on governmental programs used by many communities of color. These facts suggest that the algorithm does the opposite of following the above principle in the Data Values and Principles Manifesto. Rather than recognize and attempt to mitigate the bias within the system, the algorithm instead codifies it into the practices of the county’s child welfare system.  

Does AFST align with principle No. 9 in the Data Values and Principles Manifesto, considering carefully the ethical implications of choices made when using data, and the impacts of the work on individuals and society?

The Allegheny Family Screening Tool does not fulfill this principle when examining its effects on the families that are subject to it. One family law attorney working in Allegheny County described how her clients asked her whether the tool was screening them for whether or not they used government assistance or community programs, to which she could not give an answer (Ho & Burke, 2022). Because of the lack of transparency around how the AFST risk score is calculated, parents may find themselves anxious about participating in need-based programs with the fear that doing so will bring their families at risk for separation. Thus, developers of the tool have not considered nor mitigated the impact of their work upon individuals affected by it.

Why does this matter?

Those that benefit from the algorithm include the county and the developers involved in the creation of the algorithm. They are able to benefit financially from the contract created with the county government specifically for creating AFST. The county government benefits by arbitrarily making the work in their child welfare department “more efficient,” thus reducing the human effort required to do a thorough investigation on each report. Those that are harmed are the families that are unnecessarily subject to contact with the child welfare department because they receive a high risk score. Families may also be harmed by feeling as if they must turn down government benefits in order to avoid getting a higher score from the algorithm. 

I believe that the ethical violations occurred in the interest of surveillance. In an effort to mitigate against the harms of child neglect, the solution devised by proponents of the algorithm is to collect vast amounts of data to predict whether a child is at risk. While the motivations of the algorithm may be ethical, the fear that the algorithm creates in families who are worried about separation are a direct result of this excess surveillance. 

[References:]{.underline}

Dalton, E. (2022). DHS response to the Associated Press article “An algorithm that screens for child neglect raises concerns.” In Allegheny County. https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/dhs-response-to-ap-article_algorithm-that-screens-for-child-neglect.pdf

DHS Data Warehouse. (2024). Allegheny County. <https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/DHS-Data-Warehouse>

Ho, S., & Burke, G. (2022, April 29). *An Algorithm That Screens for Child Neglect Raises Concerns*. Pulitzer Center. <https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns>
