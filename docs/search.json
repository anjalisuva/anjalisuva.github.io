[
  {
    "objectID": "sqlpresentation.html#context",
    "href": "sqlpresentation.html#context",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Context",
    "text": "Context\nMy Question: How do the proportions of pedestrian and vehicular police stops differ across racial groups?\nFor this data set, I decided to use Los Angeles for two reasons 1) it’s very populous, and 2) the data set categorized each stop as vehicular versus pedestrian. This was a feature that other city data sets did not have.\nTo begin my exploration, I set up a SQL connection with the SQL database where Stanford Open Policing Project data was located.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)"
  },
  {
    "objectID": "sqlpresentation.html#initial-query",
    "href": "sqlpresentation.html#initial-query",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Initial Query",
    "text": "Initial Query\nNext, I aimed to create an R output (a tibble) that would display the number of pedestrian stops, vehicle stops, and total stops grouped by the race of the individual stopped. I used the SQL chunk below to parse through and create my R tibble.\n\nSELECT subject_race,\nCOUNT(*) as num_stops,\nSUM(type = 'pedestrian') AS ped_stops,\nSUM(type = 'vehicular') AS vclr_stops\nFROM ca_los_angeles_2020_04_01\nGROUP BY subject_race\nORDER BY ped_stops, vclr_stops;\n\nAfter obtaining the output, I disconnected from the SQL database\n\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)"
  },
  {
    "objectID": "sqlpresentation.html#wrangling-in-r",
    "href": "sqlpresentation.html#wrangling-in-r",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Wrangling in R",
    "text": "Wrangling in R\nOnce I had the raw counts as a data table in R, I wanted to wrangle with the data to get the proportions for each type of stop. I also decided to create a column for the difference between the proportion of individuals that experienced a vehicular stop and the proportion that experienced a pedestrian stop for each race.\n\n#create columns for proportions and differences b/t proportions\nstops_props &lt;- stops_by_type |&gt;\n  mutate(ped_prop = ped_stops/num_stops, vclr_prop = vclr_stops/num_stops) |&gt;\n  mutate(diff_prop = vclr_prop-ped_prop) |&gt;\n  select(subject_race, vclr_prop, ped_prop, diff_prop)\n\nas_tibble(stops_props)\n\n# A tibble: 5 × 4\n  subject_race           vclr_prop ped_prop diff_prop\n  &lt;chr&gt;                      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 asian/pacific islander     0.912   0.0877     0.825\n2 other                      0.902   0.0983     0.803\n3 white                      0.807   0.193      0.615\n4 black                      0.679   0.321      0.358\n5 hispanic                   0.753   0.247      0.505"
  },
  {
    "objectID": "sqlpresentation.html#graphing-the-data",
    "href": "sqlpresentation.html#graphing-the-data",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Graphing the Data",
    "text": "Graphing the Data\nIn order to graph the data easily, some more wrangling was necessary. I wanted to create a bar graph with side-by-side columns that reflected the differences between the proportions for each type of stop. I used pivot_longer to do this:\n\n#wrangle the data for graphing using pivot_longer\nstops_long &lt;- stops_props |&gt;\n  select(subject_race, ped_prop, vclr_prop) |&gt;\n  pivot_longer(cols = c(ped_prop, vclr_prop), names_to = \"stop_type\", values_to = \"proportion\")\n\n#graph data as bar graph\nggplot(stops_long, aes(x = subject_race, y = proportion, fill = stop_type)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Proportion of Stops by Race and Stop Type\",\n       x = \"Race\",\n       y = \"Proportion\",\n       fill = \"Stop Type\")"
  },
  {
    "objectID": "sqlpresentation.html#conclusions",
    "href": "sqlpresentation.html#conclusions",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the graph and stops_prop data table, here were my takeaways:\n\nBlack individuals are stopped as pedestrians at much higher rates than any other group\nAsian/Pacific Islander, Other, and White groups have very large differences b/t vehicle prop & pedestrian prop, indicating that stops are mostly vehicular for these groups\nHispanic and Black individuals face higher rates of pedestrian stops than vehicular stops compared to other racial groups, since differences b/t vehicle prop. and pedestrian prop. are smaller"
  },
  {
    "objectID": "sqlpresentation.html#implications",
    "href": "sqlpresentation.html#implications",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Implications",
    "text": "Implications\nAlthough this analysis is not enough to conclusively claim that racial bias exists within Los Angeles, the data is strongly suggestive of racial disparities within policing. A permutation test or other statistical test can be used to explore this further."
  },
  {
    "objectID": "sqlpresentation.html#data-sources",
    "href": "sqlpresentation.html#data-sources",
    "title": "SQL Traffic Stop Data Exploration",
    "section": "Data Sources",
    "text": "Data Sources\nThe Stanford Open Policing Project. (2018). Openpolicing.stanford.edu. https://openpolicing.stanford.edu/\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "childwelfareethicaldilemma.html",
    "href": "childwelfareethicaldilemma.html",
    "title": "Child Welfare Ethical Dilemma",
    "section": "",
    "text": "The Issue:\nIn Allegheny County, Pennsylvania, the Department of Human Services (DHS) utilized an algorithmic modelling tool used to predict the risk that a child will be placed in foster care two years after investigation. The Allegheny Family Screening Tool (AFST), uses personal data regarding birth, Medicaid, substance abuse, mental health, jail and probation records, among other government data sets (Ho & Burke, 2022). Using detailed personal data, the algorithm generates a risk score from 1 to 20 for each child, with 1 indicating low risk and 20 indicating very high risk. The tool is used for cases of neglect, which are separate from physical and sexual abuse and can incldue anything from inadequate housing to poor hygiene. \nCritics of the algorithm cite its encoding of racial bias as evidence of its defects. According to research done by a team at Carnegie Mellon University, AFST demonstrated a pattern of flagging black children for mandatory neglect investigations at disproportionate rates compared to white children. The team also discovered that for approximately a third of the AFST’s risk scores contradicted social workers’ assessments.\nThe United States child welfare system has a long history of discrimination against racial minorities. Several welfare workers have suggested that regardless of the developers objective attempt to assess families, racial bias inherent in the data systems used to design the algorithm will affect the way the algorithm evaluates families. \nSupporters of the algorithm include the county and the developers of the algorithm, who claim that the high stakes involved in child welfare necessitate its use. On the one hand, failing to adequately flag and investigate a child’s situation could result in death. On the other hand, unnecessarily investigating families could set them up for separation. The county and developers of AFST claim that the algorithm can help separate reports of families that require intervention from baseless reports. According to the director of the DHS, Erin Dalton, the algorithm is not meant to be the sole determinant of which family gets investigated. In a response to the article written by Ho and Burke, Dalton writes that child welfare workers consider the AFST as just one element, and that they must “weigh all the other information that is not included in the AFST such as the specific allegation, reporter, call narrative, and the screener’s assessment of risk and safety to determine the appropriate screening response” (Dalton, 2022). Thus, the final say on who gets investigated lies with the worker, not the algorithm. \nThe ethical dilemma in this project involves whether or not an algorithm meant for saving children facing neglect is causing more harm than good for the aforementioned reasons.\nQuestions/Principles to Consider:\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations?\nAccording to the Allegheny County website, the data is collected from the county’s Department of Human Services (DHS) Data Warehouse which includes publicly-funded human services data (e.g., behavioral health, child welfare, intellectual disability, homelessness and aging) and has been expanded to include other sources (DHS Data Warehouse, 2024). The sources specifically are not identified, but according to Ho and Burke (2022), one controversial element of the original version of the algorithm was that it included information on whether or not a family received welfare dollars or food stamps. The latest version of the algorithm does not include such information, but little transparency remains over what information is included. Based on the information available, the data for the algorithm is not pulled nor used with the knowing consent of the families being investigated. Although the data in general may have been collected in a neutral or ethical manner—i.e. collected when families register to receive certain resources—its specific use in the algorithm is not. The data within the several databases that the algorithm pulls from is kept private from families and the general public, so whether or not there are missing observations within the data sets is unknown. \nPrinciple No. 4 in the Data Values and Principles Manifesto: Prioritize the continuous collection and availability of discussions and metadata.\nThe Allegheny Family Screening Tool does not promote transparency as outlined in the above principle. Instead, families are kept in the dark about the role that the tool plays in investigations and how much sway it holds in court hearings. One reason for this is that family court hearings are closed to the public and records are sealed. As a result, Ho and Burke (2022) describes how they were unable to obtain first-hand accounts of families who were investigated for neglect due to the algorithm nor cases that led to a child being sent to foster care. Families are also not shown the scores that the algorithm assigns them, meaning that parents may experience a psychological toll and anxiety from being unaware of whether or not a certain action (such as signing up for government programs) may raise or lower their score. \nPrinciple No. 7 in the Data Values and Principles Manifesto: Recognize and mitigate bias in ourselves and in the data we use.\nAs demonstrated by the Carnegie Mellon University team’s research, the algorithm reinforces racial disparities within the child welfare system. The rate at which black children were flagged for mandatory neglect investigations was disproportionate compared to the rate at which white children were flagged. This is further supported by Allegheny County, which provided research confirming that AFST does not help with combatting disparities in the rates at which black and white child neglect cases are examined (Ho & Burke, 2022). Because the data utilized in the algorithm is often biased against Black families, who are more likely to be affected by discriminatory policies, the data that is utilized by the algorithms to make predictions on which children are most at risk for neglect will inherently be biased. The algorithm does not take as a variable in determining which family is at risk, but many other variables can act as a proxy for race. This includes data on families on governmental programs used by many communities of color. These facts suggest that the algorithm does the opposite of following the above principle in the Data Values and Principles Manifesto. Rather than recognize and attempt to mitigate the bias within the system, the algorithm instead codifies it into the practices of the county’s child welfare system.  \nPrinciple No. 9 in the Data Values and Principles Manifesto: Consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society.\nThe Allegheny Family Screening Tool does not fulfill this principle when examining its effects on the families that are subject to it. One family law attorney working in Allegheny County described how her clients asked her whether the tool was screening them for whether or not they used government assistance or community programs, to which she could not give an answer (Ho & Burke, 2022). Because of the lack of transparency around how the AFST risk score is calculated, parents may find themselves anxious about participating in need-based programs with the fear that doing so will bring their families at risk for separation. Thus, developers of the tool have not considered nor mitigated the impact of their work upon individuals affected by it.\nWhy does this matter?\nThose that benefit from the algorithm include the county and the developers involved in the creation of the algorithm. They are able to benefit financially from the contract created with the county government specifically for creating AFST. The county government benefits by arbitrarily making the work in their child welfare department “more efficient,” thus reducing the human effort required to do a thorough investigation on each report. Those that are harmed are the families that are unnecessarily subject to contact with the child welfare department because they receive a high risk score. Families may also be harmed by feeling as if they must turn down government benefits in order to avoid getting a higher score from the algorithm. \nI believe that the ethical violations occurred in the interest of surveillance. In an effort to mitigate against the harms of child neglect, the solution devised by proponents of the algorithm is to collect vast amounts of data to predict whether a child is at risk. While the motivations of the algorithm may be ethical, the fear that the algorithm creates in families who are worried about separation are a direct result of this excess surveillance. \nReferences:\nDalton, E. (2022). DHS response to the Associated Press article “An algorithm that screens for child neglect raises concerns.” In Allegheny County. https://www.alleghenycounty.us/files/assets/county/v/1/services/dhs/documents/allegheny-family-screening-tool/dhs-response-to-ap-article_algorithm-that-screens-for-child-neglect.pdf\nDHS Data Warehouse. (2024). Allegheny County. https://www.alleghenycounty.us/Services/Human-Services-DHS/DHS-News-and-Events/Accomplishments-and-Innovations/DHS-Data-Warehouse\nHo, S., & Burke, G. (2022, April 29). An Algorithm That Screens for Child Neglect Raises Concerns. Pulitzer Center. https://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns"
  },
  {
    "objectID": "nytheadlines.html",
    "href": "nytheadlines.html",
    "title": "NYTimes Headlines",
    "section": "",
    "text": "library(RTextTools) \nlibrary(tidyverse)\n\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows\n\n\nWith the increase in citizens obtaining their news online, I wondered if the title length for articles had shortened over time to fit these new mediums. For my first question, I wanted to investigate whether or not the average length of New York Times article titles has changed over time.\nIn order to do this analysis, I used str_length to obtain the number of characters in each article title.\n\n\n\nlibrary(lubridate)\n\n\nAvgTitleLength &lt;- NYTimes |&gt;\n  mutate(TitleLength = str_length(Title), \n         Year = year(dmy(Date))) |&gt;\n  group_by(Year) |&gt;\n  summarize(AvgTitleLength = mean(TitleLength)) |&gt;\n  select(Year, AvgTitleLength)\n\nas_tibble(AvgTitleLength)\n\n# A tibble: 11 × 2\n    Year AvgTitleLength\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1  1996           53.1\n 2  1997           45.8\n 3  1998           40.7\n 4  1999           42.5\n 5  2000           54.0\n 6  2001           54.6\n 7  2002           46.4\n 8  2003           56.9\n 9  2004           48  \n10  2005           48.4\n11  2006           46.3\n\n\n\nlibrary(ggplot2)\n\nggplot(AvgTitleLength, aes(x = Year, y = AvgTitleLength)) +\n  geom_col() +\n  labs(title = \"Average NYTimes Article Title Length by Year\",\n       x = \"Year\",\n       y = \"Average Title Length (in characters)\")\n\n\n\n\n\n\n\n\nFrom the graph above, it is clear that there is not much of a trend in the average title length over time. From 1996 to 2005, the average length of NYTimes article titles have been between 40 to 57 characters. The average length tends to fluctuate by each year, but there aren’t enough years in the data set to identify a concrete downward trend in the length of the titles.\nTaking the limited sample size for year into account, I wanted to search for something more specific—a trend that was more likely to show up on a graph. Since the years included on the dataset were from the range 1996-2005, I was interested in the number of times “Iraq” was mentioned in a headline, considering that the Iraq War began in 2003. I predict that there would be a sharp upturn in the number of times Iraq was mentioned after 2001, with 2003 being the year when it was mentioned the most.\nIn order to do this analysis, I used str_detect along with the regular expression (?i) to account for capitalized vs. non-capitalized headlines in the dataset.\n\nIraqHeadlines &lt;- NYTimes |&gt; \n  mutate(Year = year(dmy(Date))) |&gt;\n  group_by(Year) |&gt;\n  summarize(IraqinTitle = sum(str_detect(Title, \"(?i)Iraq\"))) |&gt;\n  select(Year, IraqinTitle)\n\nas_tibble(IraqHeadlines)\n\n# A tibble: 11 × 2\n    Year IraqinTitle\n   &lt;dbl&gt;       &lt;int&gt;\n 1  1996           1\n 2  1997           2\n 3  1998           5\n 4  1999           2\n 5  2000           0\n 6  2001           1\n 7  2002          16\n 8  2003          41\n 9  2004          38\n10  2005          22\n11  2006          24\n\n\n\nggplot(IraqHeadlines, aes(x = Year, y = IraqinTitle)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Number of Times \\\"Iraq\\\" Appeared in Headline by Year\",\n       x = \"Year\",\n       y = \"Number of Times \\\"Iraq\\\" Appeared in Headline\")\n\n\n\n\n\n\n\n\nAccording to the graph, there is a distinct increase in the number of times “Iraq” appears within the plot after 2001. In accordance with my prediction, the year with the greatest number of times Iraq appeared in the headline of a NYTimes article was 2003.\nSince the previous plot yielded expected results, I wondered how else I could utilize key words within the data set. While parsing through some of the headlines in the original NYTimes dataset, I noticed that certain titles would begin with a key word/phrase and a colon. For example, take this title from the dataset, “CRASH IN THE BALKANS: THE RESCUE; A Storm-Swept Mountain’s Grim Story.” The phrase “CRASH IN THE BALKANS” serves as a signifier to introduce the main subject matter of the story. I wanted to see how many of these key words or phrases existed within the data set.\nTo do this analysis, I used str_detect to filter for rows with colons and str_extract with a lookahead to extract everything up to the colon.\n\nKeyPhrasesinTitle &lt;- NYTimes |&gt;\n  filter(str_detect(Title, \":\")) |&gt;\n  mutate(KeyPhrase = str_extract(Title, \".+(?=:)\")) |&gt;\n  filter(!is.na(KeyPhrase)) |&gt;\n  group_by(KeyPhrase) |&gt;\n  summarize(KeyPhraseTotal = n()) |&gt;\n  select(KeyPhrase, KeyPhraseTotal) |&gt;\n  arrange(desc(KeyPhraseTotal))\n\n\nas_tibble(KeyPhrasesinTitle)\n\n# A tibble: 265 × 2\n   KeyPhrase              KeyPhraseTotal\n   &lt;chr&gt;                           &lt;int&gt;\n 1 A Nation Challenged                45\n 2 THE 2000 CAMPAIGN                  32\n 3 POLITICS                           21\n 4 THREATS AND RESPONSES              20\n 5 A NATION AT WAR                    17\n 6 CRISIS IN THE BALKANS              15\n 7 TESTING OF A PRESIDENT             15\n 8 AFTER THE WAR                       9\n 9 AFTEREFFECTS                        8\n10 THE STRUGGLE FOR IRAQ               7\n# ℹ 255 more rows\n\n\nAccording to the dataset, there are 265 key phrases that exist within the titles of the articles. The most common is “A Nation Challenged” with 45 instances of it in the dataset. This is followed by “THE 2000 CAMPAIGN”, which had 32 instances of it occurring.\nWhile examining the new table, I wondered if I could filter the data even more so I could find key words in the title (as opposed to phrases). For example “POLITICS” is one of the most common introductory words in the titles of articles, with 21 instances of it occurring.\nIn order to do this analysis, I slightly altered the code above to account for only words as opposed to words and phrases. To do this, I used the regular expressions of “^” and “\\w” to filter for rows that started with a word followed directly by a colon.\n\nKeyWordsinTitle &lt;- NYTimes |&gt;\n  filter(str_detect(Title, \"^\\\\w+:\")) |&gt;\n  mutate(KeyWord = str_extract(Title, \"^\\\\w+(?=:)\")) |&gt;\n  filter(!is.na(KeyWord)) |&gt;\n  group_by(KeyWord) |&gt;\n  summarize(KeyWordTotal = n()) |&gt;\n  select(KeyWord, KeyWordTotal) |&gt;\n  arrange(desc(KeyWordTotal))\n\n\nas_tibble(KeyWordsinTitle)\n\n# A tibble: 8 × 2\n  KeyWord      KeyWordTotal\n  &lt;chr&gt;               &lt;int&gt;\n1 POLITICS               24\n2 AFTEREFFECTS            8\n3 IMPEACHMENT             3\n4 BASEBALL                2\n5 AFTEREFFECT             1\n6 SUNY                    1\n7 Trial                   1\n8 Wanted                  1\n\n\nAfter doing that, I was able to view the key introductory words that were used in the titles of the headlines, which fit neatly into this 8 row table. Unsurprisingly, the most popular key word was “POLITICS”, which was easily predictable simply by examining the previous table.\nCredits:\nData: RTextTools R package: https://CRAN.R-project.org/package=RTextTools\nOriginal Source: Headlines from The New York Times, compiled by Professor Amber E. Boydstun at the University of California, Davis, https://www.amber-boydstun.com/supplementary-information-for-making-the-news.html."
  },
  {
    "objectID": "pixarfilms.html",
    "href": "pixarfilms.html",
    "title": "Pixar Films",
    "section": "",
    "text": "In this boxplot, I wanted to view the distribution of run times for Pixar films and see how long most Pixar films were. I chose to create a boxplot since I was only interested in one variable (the run time). According to the boxplot, the median run time was around 100 minutes, with 2 films as outliers. The graph is skewed to the right as a result.\nI wanted to know what the values were for the median and the first and third quartile, so I tried to wrangle the data to get the summary statistics. The tibble below shows what I found:\n\n\n# A tibble: 1 × 3\n  average_runtime    Q1    Q3\n            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1             100    95   106\n\n\nI also wanted to see what films were the two potential outliers. To do this, I created code for a data frame using the mutate function that would alert me to any outliers if the film’s run time was over 120 minutes, a metric I obtained from the boxplot above. The tibble is shown below. One of the outliers was Luca, with a run time of 151 minutes. The other film was unknown from the data set, but after some research done using the info in the release_date column, I found out that the film was Elemental.\n\n\n# A tibble: 27 × 4\n   film            run_time release_date potential_outlier\n   &lt;chr&gt;              &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;            \n 1 Luca                 151 2021-06-18   Yes              \n 2 &lt;NA&gt;                 155 2023-06-16   Yes              \n 3 Toy Story             81 1995-11-22   No               \n 4 A Bug's Life          95 1998-11-25   No               \n 5 Toy Story 2           92 1999-11-24   No               \n 6 Monsters, Inc.        92 2001-11-02   No               \n 7 Finding Nemo         100 2003-05-30   No               \n 8 The Incredibles      115 2004-11-05   No               \n 9 Cars                 117 2006-06-09   No               \n10 Ratatouille          111 2007-06-29   No               \n# ℹ 17 more rows\n\n\nCredits:\nData: TidyTuesday Pixar Films, https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-03-11/readme.md\nOriginal Source: pixarfilms R package by Eric Leung, drawing data from Wikipedia and OMDb, https://github.com/erictleung/pixarfilms"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Anjali Suva and I am a student at Pomona College majoring in Philosophy, Politics, and Economics. I am interested in working at the intersection of data science and policy. This website showcases some projects that I’ve worked on using various open-source data sets. In these projects, I’ve handled data transformations, visualizations, and permutation in R, and I’ve also explored data wrangling with SQL. Feel free to take a look around my site!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nNothing here yet!"
  },
  {
    "objectID": "cbpencounters.html",
    "href": "cbpencounters.html",
    "title": "CBP Encounters",
    "section": "",
    "text": "The graph above shows the number of Customs and Border Patrol encounters from 2020 to 2024 in different regions of the United States. The graph implies that the most recorded encounters within this time period took place in 2023, with more than 12500 encounters recorded. By simply glancing at the graph, I am compelled to conclude that the greatest proportion of encounters took place in the region(s) that belong to the “Other” category. Unfortunately, since the data is limited, we do not know what these specific areas may be.\nTo check the regional proportions more accurately, I created the graph below. This takes the bar graph above but sets the y-axis to display the proportion of encounters rather than the raw count. In this graph, it is easier to identify details specific to each region. For example, 2021 is the year where the Southwest Land Border had the greatest proportion of encounters relative to other years, and 2023/2024 are the year(s) where the Northwest Land Border had the greatest proportion of encounters relative to other years.\n\n\n\n\n\n\n\n\n\nCredits:\nData: TidyTuesday U.S. Customs and Border Protection (CBP) Encounter Data, https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-11-26/readme.md\nOriginal Source: U.S. Customs and Border Protection, https://www.cbp.gov/document/stats/nationwide-encounters"
  },
  {
    "objectID": "companyreputationscores.html",
    "href": "companyreputationscores.html",
    "title": "Company Reputation Simulation Study",
    "section": "",
    "text": "For this simulation study, I plan on taking observations found from the 2022 Axios-Harris Poll, which tracked the reputation of the most visible brands in the United States. The data set includes 100 of the most visible companies along with the industry that they operate in, their reputation score (a higher score corresponds to a better reputation) and their rank in the list.\nThe poll surveyed 33,096 Americans in a nationally representative sample, asking for “nominations” for which companies were the most well-known. Poll respondents were asked which two companies have the best reputation and which two have the worst reputation. These nominations were aggregated into a single list, counting all associated brands and subsidiaries with their respective parent companies. From this master list, the 100 most nominated companies were placed on the “Most Visible” list.\nThe “ratings” phase of the survey involved online interviews where individuals were asked to rate two companies from the “Most Visible” list with which they indicated being very or somewhat familiar.\nThrough my preliminary observation of the data set, I noticed that the company with the best reputation in the data set was Trader Joe’s, a retail store. On the other end, a few companies with low scores included Facebook and Twitter, both part of the technology industry. Since there seemed to be more retail and technology companies in the data set compared to other sectors of the industry, I wanted to see if there was a significant difference in average reputation score between the two sectors.\nTo begin the permutation test, I started by importing the data set from TidyTuesday:\n\nlibrary(tidyverse)\n\ncompanypoll &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-31/poll.csv')\n\nas_tibble(companypoll)\n\n# A tibble: 500 × 8\n   company      industry `2022_rank` `2022_rq` change  year  rank    rq\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Trader Joe's Retail             1      82.4     NA  2017    NA  NA  \n 2 Trader Joe's Retail             1      82.4     NA  2018    NA  NA  \n 3 Trader Joe's Retail             1      82.4     NA  2019    NA  78.2\n 4 Trader Joe's Retail             1      82.4     NA  2020    NA  80.7\n 5 Trader Joe's Retail             1      82.4     NA  2021    NA  NA  \n 6 HEB Grocery  Retail             2      82       NA  2017    NA  NA  \n 7 HEB Grocery  Retail             2      82       NA  2018    NA  81.1\n 8 HEB Grocery  Retail             2      82       NA  2019    NA  82.5\n 9 HEB Grocery  Retail             2      82       NA  2020    NA  83.1\n10 HEB Grocery  Retail             2      82       NA  2021    NA  NA  \n# ℹ 490 more rows\n\n\nIn the tibble above, the variable “2022_rq” refers to each company’s reputation score and the variable\n“2022_rank” assigns each company a rank based on their position in the list—as determined by reputation score in descending order.\nBecause not all the columns in this data set are particularly useful for the simulation study I hope to conduct, I will initially attempt to clean it. I want to remove the rows for companies that aren’t in the retail or technology sectors and only look at data for 2022, which are the most recent entries.\n\ncompany_poll_2022 &lt;- companypoll |&gt; \n  filter(industry == \"Retail\" | industry == \"Tech\") |&gt;\n  select(company, industry, `2022_rank`, `2022_rq`) |&gt; \n  distinct()\n\nas_tibble(company_poll_2022)\n\n# A tibble: 37 × 4\n   company        industry `2022_rank` `2022_rq`\n   &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 Trader Joe's   Retail             1      82.4\n 2 HEB Grocery    Retail             2      82  \n 3 Patagonia      Retail             3      81.8\n 4 Samsung        Tech               6      80.5\n 5 Sony           Tech              10      79.6\n 6 IBM            Tech              11      79.5\n 7 Microsoft      Tech              15      79  \n 8 The Home Depot Retail            16      78.9\n 9 Apple          Tech              21      78.6\n10 Netflix        Tech              22      78.5\n# ℹ 27 more rows\n\n\nNow that the data set has been cleaned, I plan on identifying the average reputation scores for both the retail and tech industries.\nMy Research Question: Within the United States, do popular companies in the retail industry have higher reputation scores with the American public than popular tech companies?\nMy null hypothesis is that there is no difference in average reputation scores between the retail and tech industries.\nMy alternative hypothesis is that the average reputation score in the retail industry is greater than in the tech industry.\nPart of my justification for my alternative hypothesis is based on the idea that the American public is likely to be more familiar with retail brands since they are more customer-facing (dealing in direct transactions with individual buyers). Tech companies like Facebook, on the other hand, have suffered from high profile lawsuits involving data privacy and may be negatively viewed due to public distrust or unfamiliarity with technology.\nTo begin testing my hypothesis, I want to first calculate the average reputation score between popular companies in the retail industry and the average reputation score between population companies in the technology industry. The results from the data set are below:\n\ncompany_poll_2022 |&gt; \n  filter(industry == \"Retail\" | industry == \"Tech\") |&gt;\n  group_by(industry) |&gt; \n  summarize(avg_score = mean(`2022_rq`))\n\n# A tibble: 2 × 2\n  industry avg_score\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Retail        75.2\n2 Tech          73.7\n\n\nBy merely looking at the observed values for means of the two industries, I can confirm that the average scores match my alternative hypothesis—the average score of the retail industry is higher than the score for the technology industry.\nWith these observed means, I plan on using a function to randomly reassign data points to the two different groups (retail vs. technology). Under the null hypothesis, this random reassignment should produce test statistics similar to the observed one. Using the function below, the observations in the data set are repeatedly shuffled to create multiple “null” data sets and their averages are taken. Multiple iterations of this will generate a distribution of mean differences under the null hypothesis, which can be compared to the observed difference.\n\nperm_data &lt;- function(rep, data){\n  data |&gt; \n    select(industry, `2022_rq`) |&gt; \n    mutate(score_perm = sample(`2022_rq`, replace = FALSE)) |&gt; \n    group_by(industry) |&gt; \n    summarize(obs_avg = mean(`2022_rq`),\n              perm_avg = mean(score_perm)) |&gt; \n    summarize(obs_avg_diff = diff(obs_avg),\n              perm_avg_diff = diff(perm_avg),\n              rep = rep)\n}\n\nNow that I have a permutation function, I will run it 500 times using the map function below. This will generate 500 simulated null data sets that can be used to show the distribution of permuted mean differences on a histogram, which is included below. In the histogram, the observed average differences in the original data set is shown with a red line.\n\nset.seed(50) \n\nperm_stats &lt;- \n  map(c(1:500), perm_data, data = company_poll_2022) |&gt; \n  list_rbind()\n\nperm_stats |&gt; \n  ggplot(aes(x = perm_avg_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_avg_diff), color = \"red\") +\n  labs(\n    title = \"Distribution of Permuted Mean Differences\",\n    subtitle = \"Red line: observed difference in means\",\n    x = \"Permuted Mean Difference (Retail Industry vs. Technology Industry)\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nSince the line is so close to the center of the distribution, it is highly likely that there is no real difference between the observed values and the expected difference under the null hypothesis. What is interesting to note is that although I initially expected Retail companies to have higher reputation scores, the observed mean difference was slightly negative in this distribution, indicating that Technology companies had marginally higher average reputation scores. However, this difference is small and likely due to random variation.\nTo verify that there is likely no difference between the reputation scores of the two industries, I will compute the p-value below. The code below checks whether the permuted mean difference is greater than the observed one for each permutation and takes the average of all the times where the permuted mean difference was greater than the observed one.\n\nperm_stats |&gt; \n  summarize(p_val_avg = mean(perm_avg_diff &gt; obs_avg_diff))\n\n# A tibble: 1 × 1\n  p_val_avg\n      &lt;dbl&gt;\n1     0.774\n\n\nSince the p-value is 0.774, this means that about 74.4% of the permuted differences under the null hypothesis were greater than or equal to my observed mean difference. This is a very high p-value, so with this data, I would fail to reject the null-hypothesis.\nTo recap, the steps of this study included 1) identifying the observed means between the two industries, 2) forming a null and alternative hypothesis based on observed data, 3) shuffling data to create a distribution of permuted mean differences between the two industries, 4) calculating the p-value for the proportion of permuted differences that are greater than or equal to the observed mean difference, and 5) interpreting the p-value in the context of the problem. The results of this process suggest that there is not enough evidence to conclude that the two industries differ in their reputation scores.\nSomething that is important to note is that after cleaning the data set, the number of observations were reduced very significantly to 37 observations. The limited sample size may have led to a larger p-value.\nCredits:\nData: TidyTuesday Company reputation poll, https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-05-31.\nOriginal Source: 2022 Axios-Harris Poll 100 Reputation Rankings, https://www.axios.com/2022/05/24/2022-axios-harris-poll-100-rankings."
  },
  {
    "objectID": "trafficstops.html",
    "href": "trafficstops.html",
    "title": "Traffic/Pedestrian Stops",
    "section": "",
    "text": "For this project, I plan on using the various data sets within the Stanford Open Policing Project to identify any noticeable trends or interesting features within traffic stop data for three populous cities within the United States. The data sets used in this assignment all contain standardized stop data from Nashville, Philadelphia, San Francisco, Los Angeles, and Chicago. The reason I chose Nashville, Philadelphia, and San Francisco was because I wanted to perform the same query of taking the average age of the individual subjected to a traffic stop on each of these cities to compare whether or not there was a difference in the results. These cities all have data for the age of the individual subjected to the traffic stop, which some of the other data sets were missing.\nI also chose Los Angeles and Chicago as other cities to examine because I wanted to explore data from larger cities. A useful feature of the Los Angeles data set was that the data set noted whether the the traffic stop was a vehicular or pedestrian one. Because of this, I am interested in comparing whether certain racial groups experience pedestrian stops at different rates than vehicular stops in Los Angeles.\nThe Chicago data set was unique because it had information on the officer’s age, race, sex, and years of service for each traffic stop. Given this information, I wanted to explore the relationship between the number of years of service that an officer had and whether or not they made an arrest at a traffic stop.\nTo start this project, I established a connection with the SQL database where the different data sets were located and queried the database for the various tables included in the database.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nSHOW TABLES;\n\n\nDisplaying records 1 - 10\n\n\nTables_in_traffic\n\n\n\n\nar_little_rock_2020_04_01\n\n\naz_gilbert_2020_04_01\n\n\naz_mesa_2023_01_26\n\n\naz_statewide_2020_04_01\n\n\nca_anaheim_2020_04_01\n\n\nca_bakersfield_2020_04_01\n\n\nca_long_beach_2020_04_01\n\n\nca_los_angeles_2020_04_01\n\n\nca_oakland_2020_04_01\n\n\nca_san_bernardino_2020_04_01\n\n\n\n\n\n\nDESCRIBE tn_nashville_2020_04_01;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\nprecinct\ntext\nYES\n\nNA\n\n\n\nreporting_area\ntext\nYES\n\nNA\n\n\n\nzone\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\n\n\n\nFor the first query, I wanted to examine whether there was a difference in the average age of individuals subjected to traffic stops across different racial demographics. The SQL chunk below shows the average age of individuals in Nashville sorted by race.\n\nSELECT subject_race,\n  COUNT(*) AS num_stops, \n  AVG(subject_age) AS avg_age\nFROM tn_nashville_2020_04_01\nGROUP BY subject_race\nORDER BY avg_age;\n\n\n7 records\n\n\nsubject_race\nnum_stops\navg_age\n\n\n\n\nhispanic\n164814\n33.1704\n\n\nother\n10397\n34.4970\n\n\nblack\n1165871\n36.1321\n\n\nNA\n1850\n36.2800\n\n\nunknown\n36878\n36.4913\n\n\nasian/pacific islander\n41668\n36.8812\n\n\nwhite\n1670873\n38.1401\n\n\n\n\n\nFrom the results of the data, it seems that individuals identified as Hispanic or Other had an average age that was below 35, with Hispanic individuals having the youngest average age. White individuals had the oldest average age, at 38. Although this data raises some questions, the ages are still close enough that concrete conclusions would be hard to draw. Because of this, I chose to run the same SQL chunk with a different city, Philadelphia.\n\nSELECT subject_race, \n  COUNT(*) AS num_stops, \n  AVG(subject_age) AS avg_age \nFROM pa_philadelphia_2020_04_01 \nGROUP BY subject_race\nORDER BY avg_age;\n\n\n6 records\n\n\nsubject_race\nnum_stops\navg_age\n\n\n\n\nhispanic\n184184\n33.1431\n\n\nblack\n1244249\n34.5387\n\n\nunknown\n14958\n34.7316\n\n\nwhite\n375862\n36.3299\n\n\nother\n5598\n36.9376\n\n\nasian/pacific islander\n40245\n37.2516\n\n\n\n\n\nThe data for Philadelphia show that Hispanic individuals subject to traffic stops had the youngest average age, similar to the data set for Nashville. However, the racial demographic with the oldest average age was Asian/Pacific Islander rather than White. The mid-30s age ranges within the data table appear very similar to the ones for Nashville.\nTo determine if there is a consistent pattern of Hispanic individuals having the youngest average age within the data, I decided to run the same chunk with San Francisco.\n\nSELECT subject_race, \n  COUNT(*) AS num_stops, \n  AVG(subject_age) AS avg_age \nFROM ca_san_francisco_2020_04_01\nGROUP BY subject_race\nORDER BY avg_age;\n\n\n5 records\n\n\nsubject_race\nnum_stops\navg_age\n\n\n\n\nhispanic\n116014\n34.7626\n\n\nblack\n152196\n35.9594\n\n\nother\n106858\n36.9774\n\n\nasian/pacific islander\n157684\n38.9475\n\n\nwhite\n372318\n39.2809\n\n\n\n\n\nSimilar to the previous two data queries, the query for San Francisco shows that Hispanic individuals subject to traffic stops had the youngest average age of all the racial demographics included in the data. The data for San Francisco also show that White individuals had the oldest average age, 39.\nAlthough the pattern is interesting to note, the data by itself cannot be used to conclusively prove differences in law enforcement interactions with individuals of different races. More analysis is required for this conclusion.\nI was interested in exploring more patterns related to race in the traffic stop data, so for the next query I wanted to compare whether certain racial groups experience pedestrian stops at different rates than vehicular stops. For this data set, I decided to use Los Angeles because it is one of the most populous cities in the US, and because the data set categorized each stop as vehicular versus pedestrian—a feature that other city data sets did not have. I determined this through the query below, which uses SELECT DISTINCT to return only unique rows for the “Type” column.\n\nSELECT DISTINCT type \nFROM ca_los_angeles_2020_04_01\nLIMIT 0, 10;\n\n\n3 records\n\n\ntype\n\n\n\n\npedestrian\n\n\nvehicular\n\n\nNA\n\n\n\n\n\n\nSELECT subject_race,\nCOUNT(*) as num_stops,\nSUM(type = 'pedestrian') AS ped_stops,\nSUM(type = 'vehicular') AS vclr_stops\nFROM ca_los_angeles_2020_04_01\nGROUP BY subject_race\nORDER BY ped_stops, vclr_stops;\n\n\nstops_by_type\n\n            subject_race num_stops ped_stops vclr_stops\n1 asian/pacific islander    205561     18026     187535\n2                  other    337342     33167     304175\n3                  white   1275788    245705    1030083\n4                  black   1297885    416681     881203\n5               hispanic   2301826    569469    1732357\n\n\n\nstops_by_type_adj &lt;- stops_by_type |&gt;\n  pivot_longer(cols = c(ped_stops, vclr_stops), names_to = \"stop_type\", values_to = \"count\")\n\nggplot(stops_by_type_adj, aes(x = subject_race, y = count, fill = stop_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Stops by Race and Stop Type in Los Angeles\",\n    x = \"Subject Race\",\n    y = \"Number of Stops\",\n    fill = \"Stop Type\")\n\n\n\n\n\n\n\n\nThe data for Los Angeles has some interesting features. One thing that I immediately noticed with the data is that although the number of White individuals who were subject to a vehicular stop is greater than the number of Black individuals (1030083 &gt; 881203), the reverse was true for pedestrian stops. Black individuals experienced more pedestrian stops than White individuals. Additionally, the proportion of pedestrian stops to vehicular stops also varied quite wildly between racial groups. The Asian/Pacific Islander demographic had the lowest proportion of 0.096. This was followed by the Other demographic, with a proportion of 0.109. The proportion of pedestrian stops to vehicular stops for White individuals was also noticeably lower, 0.239. However, the ratio for black individuals was 0.473, almost 1/2. For Hispanic individuals, the proportion was 0.329.\nThese numbers suggest that Black and Hispanic individuals may experience more pedestrian stops compared to other racial groups. This may explain the discrepancy between the pedestrian and vehicular stops for black individuals, since it may be harder for law enforcement to identify an individual’s race while they are driving than when they are walking.\nI wanted to continue exploring the data sets, so I decided to examine another populous city, Chicago. While examining the Chicago data set, I realized that the data set had information on the officer’s age, race, sex, and years of service for each traffic stop. With these extra pieces of information, I wanted to explore the relationship between the number of years of service that an officer had and whether or not they made an arrest at a traffic stop.\n\nDESCRIBE il_chicago_2023_01_26;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\ngeocode_source\ntext\nYES\n\nNA\n\n\n\nbeat\ntext\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\n\n\n\n\nSELECT officer_years_of_service, \n  COUNT(*) AS arrests\nFROM il_chicago_2023_01_26\nWHERE arrest_made = 1\nGROUP BY officer_years_of_service\nORDER BY officer_years_of_service;\n\n\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)\n\nI used the query above to create an R object with two columns: a column for year of service and a column for the number of arrests made by each officer having that many years of service.\n\narrests_by_service_year\n\n   officer_years_of_service arrests\n1                        NA   60043\n2                         0    3387\n3                         1   16927\n4                         2   13886\n5                         3    9726\n6                         4    7325\n7                         5    6677\n8                         6    7355\n9                         7    8233\n10                        8    7800\n11                        9    8168\n12                       10    7306\n13                       11    6847\n14                       12    6354\n15                       13    5430\n16                       14    4910\n17                       15    5125\n18                       16    4574\n19                       17    3741\n20                       18    3095\n21                       19    2302\n22                       20    2150\n23                       21    1967\n24                       22    1959\n25                       23    1397\n26                       24    1089\n27                       25     927\n28                       26     741\n29                       27     589\n30                       28     459\n31                       29     327\n32                       30     220\n33                       31     128\n34                       32     144\n35                       33     226\n36                       34     239\n37                       35     131\n38                       36      76\n39                       37      73\n40                       38       7\n41                       39       1\n42                       40       1\n43                       42      41\n44                       43      19\n45                       44       5\n46                       45      33\n47                       46      35\n48                       47      21\n49                       48       6\n50                       49      12\n51                      110       2\n52                      111       1\n53                     1018       1\n54                     1815       4\n55                     1816       7\n\n\nWhen examining the data in the table, I noticed that there was something strange with some of the entries under the column “officer_years_of_service.” There were about five entries in the column that were above 100 years, which are impossible numbers. In order to graph the data more accurately, I decided to filter those impossible rows before graphing.\nAlthough the table has around 50 rows, several observations can be made just by parsing through the rows. Something of note is that the greatest number of arrests occurred by officers with 1 year of service. After one year, the number of arrests appears to decrease sharply and gradually decrease as the number of years of service increases. In order to see this trend more easily, I decided to graph the table with ggplot as a line graph.\n\narrests_by_service_year_clean &lt;- arrests_by_service_year |&gt;\n  filter(officer_years_of_service &lt; 100)\n\nggplot(arrests_by_service_year_clean, aes(x = officer_years_of_service, y = arrests)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Arrests by Officer Years of Service\",\n  x = \"Officer Years of Service\",\n  y = \"Number of Arrests\") \n\n\n\n\n\n\n\n\nThe data in the graph appears to confirm my observations about the table. The peak number of arrests were made by officers with one year of service, and as officers have more years of service, the arrests gradually decline.\nThis relationship may be due to a variety of factors, all of which cannot be proved by the data set alone but may be worth exploring. For one, there may simply be more junior officers in general, and less officers with greater years of experience. Younger officers may be more active in patrolling, causing greater numbers of arrests. Older officers may also be more selective about traffic stops or more cautious about entering potentially dangerous situations which may warrant arrests. Although these reasons are mostly speculation, the trend is interesting because it is very pronounced.\nIn conclusion, the exploration of the data above aligns with existing understanding about racial bias within policing and the disproportionate targeting of certain racial demographics over others. While the data tables and graphs above are not enough to conclusively prove the existence of racial bias in policing, there is evidence of differences between racial groups in their interactions with law enforcement.\nCredits:\nThe Stanford Open Policing Project. (2018). Openpolicing.stanford.edu. https://openpolicing.stanford.edu/\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  }
]