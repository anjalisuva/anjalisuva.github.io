[
  {
    "objectID": "companyreputationscores.html",
    "href": "companyreputationscores.html",
    "title": "Company Reputation Simulation Study",
    "section": "",
    "text": "For this simulation study, I plan on taking observations found from the 2022 Axios-Harris Poll, which tracked the reputation of the most visible brands in the United States. The data set includes 100 of the most visible companies along with the industry that they operate in, their reputation score (a higher score corresponds to a better reputation) and their rank in the list.\nThrough my preliminary observation of the data set, I noticed that the company with the best reputation in the data set was Trader Joe’s, a retail store. On the other end, a few companies with low scores included Facebook and Twitter, both part of the technology industry. Since there seemed to be more retail and technology companies in the data set compared to other sectors of the industry, I wanted to see if there was a significant difference in average reputation score between the two sectors.\nTo begin the permutation test, I started by importing the data set from TidyTuesday:\n\nlibrary(tidyverse)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-31')\ncompanypoll &lt;- tuesdata$poll\n\nas_tibble(companypoll)\n\n# A tibble: 500 × 8\n   company      industry `2022_rank` `2022_rq` change  year  rank    rq\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Trader Joe's Retail             1      82.4     NA  2017    NA  NA  \n 2 Trader Joe's Retail             1      82.4     NA  2018    NA  NA  \n 3 Trader Joe's Retail             1      82.4     NA  2019    NA  78.2\n 4 Trader Joe's Retail             1      82.4     NA  2020    NA  80.7\n 5 Trader Joe's Retail             1      82.4     NA  2021    NA  NA  \n 6 HEB Grocery  Retail             2      82       NA  2017    NA  NA  \n 7 HEB Grocery  Retail             2      82       NA  2018    NA  81.1\n 8 HEB Grocery  Retail             2      82       NA  2019    NA  82.5\n 9 HEB Grocery  Retail             2      82       NA  2020    NA  83.1\n10 HEB Grocery  Retail             2      82       NA  2021    NA  NA  \n# ℹ 490 more rows\n\n\nBecause not all the columns in this data set are particularly useful for the simulation study I hope to conduct, I am going to first attempt to clean it. I want to remove the rows for companies that aren’t in the retail or technology sectors and only look at data for 2022, which are the most recent entries.\n\ncompany_poll_2022 &lt;- companypoll |&gt; \n  filter(industry == \"Retail\" | industry == \"Tech\") |&gt;\n  select(company, industry, `2022_rank`, `2022_rq`) |&gt; \n  distinct()\n\nas_tibble(company_poll_2022)\n\n# A tibble: 37 × 4\n   company        industry `2022_rank` `2022_rq`\n   &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 Trader Joe's   Retail             1      82.4\n 2 HEB Grocery    Retail             2      82  \n 3 Patagonia      Retail             3      81.8\n 4 Samsung        Tech               6      80.5\n 5 Sony           Tech              10      79.6\n 6 IBM            Tech              11      79.5\n 7 Microsoft      Tech              15      79  \n 8 The Home Depot Retail            16      78.9\n 9 Apple          Tech              21      78.6\n10 Netflix        Tech              22      78.5\n# ℹ 27 more rows\n\n\nNow that the data set has been cleaned, I plan on identifying the average reputation scores for both the retail and tech industries.\nMy Research Question: Within the United States, do popular companies in the retail industry have higher reputation scores with the American public than popular tech companies?\nMy null hypothesis is that there is no difference in average reputation scores between the retail and tech industries.\nMy alternative hypothesis is that the average reputation score in the retail industry is greater than in the tech industry.\nPart of my justification for my alternative hypothesis is based on the idea that the American public is likely to be more familiar with retail brands since they are more customer-facing (dealing in direct transactions with individual buyers). Tech companies like Facebook on the other hand, have suffered from high profile lawsuits involving data privacy and may be negatively viewed due to public distrust or unfamiliarity with technology.\nTo begin testing my hypothesis, I want to first calculate the average reputation score between popular companies in the retail industry and the average reputation score between population companies in the technology industry. The results from the data set are below:\n\ncompany_poll_2022 |&gt; \n  filter(industry == \"Retail\" | industry == \"Tech\") |&gt;\n  group_by(industry) |&gt; \n  summarize(avg_score = mean(`2022_rq`))\n\n# A tibble: 2 × 2\n  industry avg_score\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Retail        75.2\n2 Tech          73.7\n\n\nBy merely looking at the observed values for means of the two industries, I can confirm that the average scores match my alternative hypothesis—the average score of the retail industry is higher than the score for the technology industry.\nWith these observed means, I plan on using a function to randomly reassign data points to the two different groups (retail vs. technology). Under the null hypothesis, this random reassignment should produce test statistics similar to the observed one. Using the function below, the observations in the data set are repeatedly shuffled to create multiple “null” data sets and their averages are taken. Multiple iterations of this will generate a distribution of mean differences under the null hypothesis, which can be compared to the observed difference.\n\nperm_data &lt;- function(rep, data){\n  data |&gt; \n    select(industry, `2022_rq`) |&gt; \n    mutate(score_perm = sample(`2022_rq`, replace = FALSE)) |&gt; \n    group_by(industry) |&gt; \n    summarize(obs_avg = mean(`2022_rq`),\n              perm_avg = mean(score_perm)) |&gt; \n    summarize(obs_avg_diff = diff(obs_avg),\n              perm_avg_diff = diff(perm_avg),\n              rep = rep)\n}\n\nNow that I have a permutation function, I will run it 500 times using the map function below. This will generate 500 simulated null data sets that can be used to show the distribution of permuted mean differences on a histogram, which is included below. In the histogram, the observed average differences in the original data set is shown with a red line.\n\nset.seed(50) \n\nperm_stats &lt;- \n  map(c(1:500), perm_data, data = company_poll_2022) |&gt; \n  list_rbind()\n\nperm_stats |&gt; \n  ggplot(aes(x = perm_avg_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_avg_diff), color = \"red\") +\n  labs(\n    title = \"Distribution of Permuted Mean Differences\",\n    subtitle = \"Red line: observed difference in means\",\n    x = \"Permuted Mean Difference (Retail Industry vs. Technology Industry)\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nSince the line is so close to the center of the distribution, it is highly likely that there is no real difference between the observed values and the expected difference under the null hypothesis. What is interesting to note is that although I initially expected Retail companies to have higher reputation scores, the observed mean difference was slightly negative in this distribution, indicating that Technology companies had marginally higher average reputation scores. However, this difference is small and likely due to random variation.\nTo verify that there is likely no difference between the reputation scores of the two industries, I will compute the p-value below. The code below checks whether the permuted mean difference is greater than the observed one for each permutation and takes the average of all the times where the permuted mean difference was greater than the observed one.\n\nperm_stats |&gt; \n  summarize(p_val_avg = mean(perm_avg_diff &gt; obs_avg_diff))\n\n# A tibble: 1 × 1\n  p_val_avg\n      &lt;dbl&gt;\n1     0.774\n\n\nSince the p-value is 0.774, this means that about 74.4% of the permuted differences under the null hypothesis were greater than or equal to my observed mean difference. This is a very high p-value, so with this data, I would fail to reject the null-hypothesis.\nTo recap, the steps of this study included 1) identifying the observed means between the two industries, 2) forming a null and alternative hypothesis based on observed data, 3) shuffling data to create a distribution of permuted mean differences between the two industries, 4) calculating the p-value for the proportion of permuted differences that are greater than or equal to the observed mean difference, and 5) interpreting the p-value in the context of the problem. The results of this process suggest that there is not enough evidence to conclude that the two industries differ in their reputation scores.\nSomething that is important to note is that after cleaning the data set, the number of observations were reduced very significantly to 37 observations. The limited sample size may have led to a larger p-value.\nCredits:\nData: TidyTuesday Company reputation poll, https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-05-31.\nOriginal Source: 2022 Axios-Harris Poll 100 Reputation Rankings, https://www.axios.com/2022/05/24/2022-axios-harris-poll-100-rankings."
  },
  {
    "objectID": "cbpencounters.html",
    "href": "cbpencounters.html",
    "title": "CBP Encounters",
    "section": "",
    "text": "# A tibble: 6 × 9\n  fiscal_year month_grouping month_abbv land_border_region   state demographic  \n        &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;        \n1        2020 FYTD           APR        Northern Land Border ID    Single Adults\n2        2020 FYTD           APR        Northern Land Border ME    Single Adults\n3        2020 FYTD           APR        Northern Land Border ME    Single Adults\n4        2020 FYTD           APR        Northern Land Border ME    Single Adults\n5        2020 FYTD           APR        Northern Land Border ME    Single Adults\n6        2020 FYTD           APR        Northern Land Border ME    Single Adults\n# ℹ 3 more variables: citizenship &lt;chr&gt;, title_of_authority &lt;chr&gt;,\n#   encounter_count &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\nThe graph above shows the number of Customs and Border Patrol encounters from 2020 to 2024 in different regions of the United States. The graph implies that the most recorded encounters within this time period took place in 2023, with more than 12500 encounters recorded. By simply glancing at the graph, I am compelled to conclude that the greatest proportion of encounters took place in the region(s) that belong to the “Other” category. Unfortunately, since the data is limited, we do not know what these specific areas may be.\nTo check the regional proportions more accurately, I created the graph below. This takes the bar graph above but sets the y-axis to display the proportion of encounters rather than the raw count. In this graph, it is easier to identify details specific to each region. For example, 2021 is the year where the Southwest Land Border had the greatest proportion of encounters relative to other years, and 2023/2024 are the year(s) where the Northwest Land Border had the greatest proportion of encounters relative to other years.\n\n\n\n\n\n\n\n\n\nCredits:\nData: TidyTuesday U.S. Customs and Border Protection (CBP) Encounter Data, https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-11-26/readme.md\nOriginal Source: U.S. Customs and Border Protection, https://www.cbp.gov/document/stats/nationwide-encounters"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nNothing here yet!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My first website!",
    "section": "",
    "text": "Hello! My name is Anjali Suva and this is a website I am developing for my data science class. I am a junior at Pomona College and I am majoring in Politics, Philosophy, and Economics."
  },
  {
    "objectID": "pixarfilms.html",
    "href": "pixarfilms.html",
    "title": "Pixar Films",
    "section": "",
    "text": "# A tibble: 6 × 5\n  number film            release_date run_time film_rating\n   &lt;dbl&gt; &lt;chr&gt;           &lt;date&gt;          &lt;dbl&gt; &lt;chr&gt;      \n1      1 Toy Story       1995-11-22         81 G          \n2      2 A Bug's Life    1998-11-25         95 G          \n3      3 Toy Story 2     1999-11-24         92 G          \n4      4 Monsters, Inc.  2001-11-02         92 G          \n5      5 Finding Nemo    2003-05-30        100 G          \n6      6 The Incredibles 2004-11-05        115 PG         \n\n\n\n\n\n\n\n\n\nIn this boxplot, I wanted to view the distribution of run times for Pixar films and see how long most Pixar films were. I chose to create a boxplot since I was only interested in one variable (the run time). According to the boxplot, the median run time was around 100 minutes, with 2 films as outliers. The graph is skewed to the right as a result.\nI wanted to know what the values were for the median and the first and third quartile, so I tried to wrangle the data to get the summary statistics. The tibble below shows what I found:\n\n\n# A tibble: 1 × 3\n  average_runtime    Q1    Q3\n            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1             100    95   106\n\n\nI also wanted to see what films were the two potential outliers. To do this, I created code for a data frame using the mutate function that would alert me to any outliers if the film’s run time was over 120 minutes, a metric I obtained from the boxplot above. The tibble is shown below. One of the outliers was Luca, with a run time of 151 minutes. The other film was unknown from the data set, but after some research done using the info in the release_date column, I found out that the film was Elemental.\n\n\n# A tibble: 27 × 4\n   film            run_time release_date potential_outlier\n   &lt;chr&gt;              &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;            \n 1 Luca                 151 2021-06-18   Yes              \n 2 &lt;NA&gt;                 155 2023-06-16   Yes              \n 3 Toy Story             81 1995-11-22   No               \n 4 A Bug's Life          95 1998-11-25   No               \n 5 Toy Story 2           92 1999-11-24   No               \n 6 Monsters, Inc.        92 2001-11-02   No               \n 7 Finding Nemo         100 2003-05-30   No               \n 8 The Incredibles      115 2004-11-05   No               \n 9 Cars                 117 2006-06-09   No               \n10 Ratatouille          111 2007-06-29   No               \n# ℹ 17 more rows\n\n\nCredits:\nData: TidyTuesday Pixar Films, https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-03-11/readme.md\nOriginal Source: pixarfilms R package by Eric Leung, drawing data from Wikipedia and OMDb, https://github.com/erictleung/pixarfilms"
  },
  {
    "objectID": "nytheadlines.html",
    "href": "nytheadlines.html",
    "title": "NYTimes Headlines",
    "section": "",
    "text": "library(RTextTools) \nlibrary(tidyverse)\n\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows\n\n\nWith the proliferation of the internet and increase in citizens obtaining their news online, I wondered if the title length for articles had shortened over time to fit these new mediums. For my first question, I wanted to investigate whether or not the average length of NY Times article titles has changed over time.\nIn order to do this analysis, I used str_length to obtain the number of characters in each article title.\n\n\n\nlibrary(lubridate)\n\n\nAvgTitleLength &lt;- NYTimes |&gt;\n  mutate(TitleLength = str_length(Title), \n         Year = year(dmy(Date))) |&gt;\n  group_by(Year) |&gt;\n  summarize(AvgTitleLength = mean(TitleLength)) |&gt;\n  select(Year, AvgTitleLength)\n\nas_tibble(AvgTitleLength)\n\n# A tibble: 11 × 2\n    Year AvgTitleLength\n   &lt;dbl&gt;          &lt;dbl&gt;\n 1  1996           53.1\n 2  1997           45.8\n 3  1998           40.7\n 4  1999           42.5\n 5  2000           54.0\n 6  2001           54.6\n 7  2002           46.4\n 8  2003           56.9\n 9  2004           48  \n10  2005           48.4\n11  2006           46.3\n\n\n\nlibrary(ggplot2)\n\nggplot(AvgTitleLength, aes(x = Year, y = AvgTitleLength)) +\n  geom_col() +\n  labs(title = \"Average NYTimes Article Title Length by Year\",\n       x = \"Year\",\n       y = \"Average Title Length (in characters)\")\n\n\n\n\n\n\n\n\nFrom the graph above, it is clear that there is not much of a trend in the average title length over time. From 1996 to 2005, the average length of NYTimes article titles have been between 40 to 57 characters. The average length tends to fluctuate by each year, but there aren’t enough years in the data set to identify a concrete downward trend in the length of the titles.\nTaking the limited sample size for year into account, I wanted to search for something more specific—a trend that was more likely to show up on a graph. Since the years included on the dataset were from the range 1996-2005, I was interested in the number of times “Iraq” was mentioned in a headline, considering that the Iraq War began in 2003. I predict that there would be a sharp upturn in the number of times Iraq was mentioned after 2001, with 2003 being the year when it was mentioned the most.\nIn order to do this analysis, I used str_detect along with the regular expression (?i) to account for capitalized vs. non-capitalized headlines in the dataset.\n\nIraqHeadlines &lt;- NYTimes |&gt; \n  mutate(Year = year(dmy(Date))) |&gt;\n  group_by(Year) |&gt;\n  summarize(IraqinTitle = sum(str_detect(Title, \"(?i)Iraq\"))) |&gt;\n  select(Year, IraqinTitle)\n\nas_tibble(IraqHeadlines)\n\n# A tibble: 11 × 2\n    Year IraqinTitle\n   &lt;dbl&gt;       &lt;int&gt;\n 1  1996           1\n 2  1997           2\n 3  1998           5\n 4  1999           2\n 5  2000           0\n 6  2001           1\n 7  2002          16\n 8  2003          41\n 9  2004          38\n10  2005          22\n11  2006          24\n\n\n\nggplot(IraqHeadlines, aes(x = Year, y = IraqinTitle)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Number of Times \\\"Iraq\\\" Appeared in Headline by Year\",\n       x = \"Year\",\n       y = \"Number of Times \\\"Iraq\\\" Appeared in Headline\")\n\n\n\n\n\n\n\n\nAccording to the graph, there is a distinct increase in the number of times “Iraq” appears within the plot after 2001. In accordance with my prediction, the year with the greatest number of times Iraq appeared in the headline of a NYTimes article was 2003.\nSince the previous plot yielded expected results, I wondered how else I could utilize key words within the data set. While parsing through some of the headlines in the original NYTimes dataset, I noticed that certain titles would begin with a key word/phrase and a colon. For example, take this title from the dataset, “CRASH IN THE BALKANS: THE RESCUE; A Storm-Swept Mountain’s Grim Story.” The phrase “CRASH IN THE BALKANS” serves as a signifier to introduce the main subject matter of the story. I wanted to see how many of these key words or phrases existed within the data set.\nTo do this analysis, I used str_detect to filter for rows with colons and str_extract with a lookahead to extract the key phrases I wanted.\n\nKeyPhrasesinTitle &lt;- NYTimes |&gt;\n  filter(str_detect(Title, \":\")) |&gt;\n  mutate(KeyPhrase = str_extract(Title, \".+(?=:)\")) |&gt;\n  filter(!is.na(KeyPhrase)) |&gt;\n  group_by(KeyPhrase) |&gt;\n  summarize(KeyPhraseTotal = n()) |&gt;\n  select(KeyPhrase, KeyPhraseTotal) |&gt;\n  arrange(desc(KeyPhraseTotal))\n\n\nas_tibble(KeyPhrasesinTitle)\n\n# A tibble: 265 × 2\n   KeyPhrase              KeyPhraseTotal\n   &lt;chr&gt;                           &lt;int&gt;\n 1 A Nation Challenged                45\n 2 THE 2000 CAMPAIGN                  32\n 3 POLITICS                           21\n 4 THREATS AND RESPONSES              20\n 5 A NATION AT WAR                    17\n 6 CRISIS IN THE BALKANS              15\n 7 TESTING OF A PRESIDENT             15\n 8 AFTER THE WAR                       9\n 9 AFTEREFFECTS                        8\n10 THE STRUGGLE FOR IRAQ               7\n# ℹ 255 more rows\n\n\nAccording to the dataset, there are 265 key phrases that exist within the titles of the articles. The most common is “A Nation Challenged” with 45 instances of it in the dataset. This is followed by “THE 2000 CAMPAIGN”, which had 32 instances of it occurring.\nWhile examining the new table, I wondered if I could filter the data even more so I could find key words in the title (as opposed to phrases). For example “POLITICS” is one of the most common introductory words in the titles of articles, with 21 instances of it occurring.\nIn order to do this analysis, I slightly altered the code above to account for only words as opposed to words and phrases. To do this, I used the regular expressions of “^” and “\\w” to filter for rows that started with a word followed directly by a colon.\n\nKeyWordsinTitle &lt;- NYTimes |&gt;\n  filter(str_detect(Title, \"^\\\\w+:\")) |&gt;\n  mutate(KeyWord = str_extract(Title, \"^\\\\w+(?=:)\")) |&gt;\n  filter(!is.na(KeyWord)) |&gt;\n  group_by(KeyWord) |&gt;\n  summarize(KeyWordTotal = n()) |&gt;\n  select(KeyWord, KeyWordTotal) |&gt;\n  arrange(desc(KeyWordTotal))\n\n\nas_tibble(KeyWordsinTitle)\n\n# A tibble: 8 × 2\n  KeyWord      KeyWordTotal\n  &lt;chr&gt;               &lt;int&gt;\n1 POLITICS               24\n2 AFTEREFFECTS            8\n3 IMPEACHMENT             3\n4 BASEBALL                2\n5 AFTEREFFECT             1\n6 SUNY                    1\n7 Trial                   1\n8 Wanted                  1\n\n\nAfter doing that, I was able to view the key introductory words that were used in the titles of the headlines, which fit neatly into this 8 row table. Unsurprisingly, the most popular key word was “POLITICS”, which was easily predictable simply by examining the previous table.\nCredits:\nData: RTextTools R package: https://CRAN.R-project.org/package=RTextTools\nOriginal Source: Headlines from The New York Times, compiled by Professor Amber E. Boydstun at the University of California, Davis, https://www.amber-boydstun.com/supplementary-information-for-making-the-news.html."
  }
]